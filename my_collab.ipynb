{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f99a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "import io\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For Fourier analysis\n",
    "from scipy.fft import fft2, fftshift\n",
    "from skimage import filters, transform\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Use notebook-internal class definitions only\n",
    "USE_EXTERNAL_CLASSES = False\n",
    "\n",
    "print(\"All imports successful - using notebook-internal classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda3d219",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningParquetProcessor:\n",
    "    \"\"\"Optimized parquet processing for 4000+ websites\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_parquet_fast(file_path: str, sample_size: Optional[int] = None) -> pd.DataFrame:\n",
    "        \"\"\"Load parquet with PyArrow for maximum speed\"\"\"\n",
    "        print(f\"Loading parquet: {file_path}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Use PyArrow for fastest loading\n",
    "        import pyarrow.parquet as pq\n",
    "        table = pq.read_table(file_path)\n",
    "        df = table.to_pandas()\n",
    "        \n",
    "        # Sample if requested\n",
    "        if sample_size and len(df) > sample_size:\n",
    "            df = df.sample(n=sample_size, random_state=42)\n",
    "            print(f\" Sampled {sample_size} from {len(table)} total websites\")\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\" Loaded {len(df)} websites in {elapsed:.2f}s\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_website_column(df: pd.DataFrame) -> str:\n",
    "        \"\"\"Auto-detect website column\"\"\"\n",
    "        website_cols = ['website', 'url', 'domain', 'site', 'link']\n",
    "        for col in website_cols:\n",
    "            if col in df.columns:\n",
    "                return col\n",
    "        \n",
    "        # Check for columns containing 'web' or 'url'\n",
    "        for col in df.columns:\n",
    "            if any(term in col.lower() for term in ['web', 'url', 'domain']):\n",
    "                return col\n",
    "        \n",
    "        # Default to first column\n",
    "        return df.columns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc071e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedAPILogoExtractor:\n",
    "    \"\"\"Enhanced logo extraction with massive API pool + DNS discovery for 98%+ success rate\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.session = None\n",
    "        # MEGA-EXPANDED API pool - 49 services across 8 tiers including DNS discovery\n",
    "        self.logo_apis = [\n",
    "            # Tier 1: Premium/Fast APIs (Highest quality, fastest)\n",
    "            {\n",
    "                'name': 'Clearbit',\n",
    "                'url': 'https://logo.clearbit.com/{domain}',\n",
    "                'params': {},\n",
    "                'headers': {},\n",
    "                'timeout': 3,\n",
    "                'tier': 1\n",
    "            },\n",
    "            {\n",
    "                'name': 'LogoAPI',\n",
    "                'url': 'https://api.logo.dev/{domain}',\n",
    "                'params': {},\n",
    "                'headers': {},\n",
    "                'timeout': 4,\n",
    "                'tier': 1\n",
    "            },\n",
    "            {\n",
    "                'name': 'BrandAPI',\n",
    "                'url': 'https://logo.api.brand.io/{domain}',\n",
    "                'params': {},\n",
    "                'headers': {},\n",
    "                'timeout': 4,\n",
    "                'tier': 1\n",
    "            },\n",
    "            {\n",
    "                'name': 'Brandfetch',\n",
    "                'url': 'https://api.brandfetch.io/v2/brands/{domain}',\n",
    "                'params': {},\n",
    "                'headers': {},\n",
    "                'timeout': 4,\n",
    "                'tier': 1\n",
    "            },\n",
    "            {\n",
    "                'name': 'LogoGrab',\n",
    "                'url': 'https://api.logograb.com/v1/logo/{domain}',\n",
    "                'params': {},\n",
    "                'headers': {},\n",
    "                'timeout': 4,\n",
    "                'tier': 1\n",
    "            },\n",
    "            \n",
    "            # Tier 2: Google & Microsoft Services (Very reliable)\n",
    "            {\n",
    "                'name': 'Google Favicon',\n",
    "                'url': 'https://www.google.com/s2/favicons',\n",
    "                'params': {'domain': '{domain}', 'sz': '128'},\n",
    "                'headers': {},\n",
    "                'timeout': 2,\n",
    "                'tier': 2\n",
    "            },\n",
    "            {\n",
    "                'name': 'Google Favicon HD',\n",
    "                'url': 'https://www.google.com/s2/favicons',\n",
    "                'params': {'domain': '{domain}', 'sz': '256'},\n",
    "                'headers': {},\n",
    "                'timeout': 3,\n",
    "                'tier': 2\n",
    "            },\n",
    "            {\n",
    "                'name': 'Google Favicon XL',\n",
    "                'url': 'https://www.google.com/s2/favicons',\n",
    "                'params': {'domain': '{domain}', 'sz': '512'},\n",
    "                'headers': {},\n",
    "                'timeout': 3,\n",
    "                'tier': 2\n",
    "            },\n",
    "            {\n",
    "                'name': 'Microsoft Bing',\n",
    "                'url': 'https://www.bing.com/th',\n",
    "                'params': {'id': 'OIP.{domain}', 'w': '128', 'h': '128', 'c': '7', 'r': '0', 'o': '5'},\n",
    "                'headers': {},\n",
    "                'timeout': 4,\n",
    "                'tier': 2\n",
    "            },\n",
    "            {\n",
    "                'name': 'DuckDuckGo Favicon',\n",
    "                'url': 'https://icons.duckduckgo.com/ip3/{domain}.ico',\n",
    "                'params': {},\n",
    "                'headers': {},\n",
    "                'timeout': 3,\n",
    "                'tier': 2\n",
    "            },\n",
    "            \n",
    "            # Tier 3: Alternative Favicon Services & CDNs\n",
    "            {\n",
    "                'name': 'Favicon.io',\n",
    "                'url': 'https://favicons.githubusercontent.com/{domain}',\n",
    "                'params': {},\n",
    "                'headers': {},\n",
    "                'timeout': 3,\n",
    "                'tier': 3\n",
    "            },\n",
    "            {\n",
    "                'name': 'Icons8',\n",
    "                'url': 'https://img.icons8.com/color/128/{domain}',\n",
    "                'params': {},\n",
    "                'headers': {},\n",
    "                'timeout': 4,\n",
    "                'tier': 3\n",
    "            },\n",
    "            {\n",
    "                'name': 'Favicon Kit',\n",
    "                'url': 'https://www.faviconkit.com/{domain}/128',\n",
    "                'params': {},\n",
    "                'headers': {},\n",
    "                'timeout': 3,\n",
    "                'tier': 3\n",
    "            },\n",
    "            {\n",
    "                'name': 'Favicon Grabber',\n",
    "                'url': 'https://favicongrabber.com/api/grab/{domain}',\n",
    "                'params': {},\n",
    "                'headers': {},\n",
    "                'timeout': 4,\n",
    "                'tier': 3\n",
    "            },\n",
    "            {\n",
    "                'name': 'GetFavicon',\n",
    "                'url': 'https://getfavicon.appspot.com/{domain}',\n",
    "                'params': {},\n",
    "                'headers': {},\n",
    "                'timeout': 3,\n",
    "                'tier': 3\n",
    "            },\n",
    "            {\n",
    "                'name': 'Besticon',\n",
    "                'url': 'https://besticon-demo.herokuapp.com/icon',\n",
    "                'params': {'url': 'https://{domain}', 'size': '128'},\n",
    "                'headers': {},\n",
    "                'timeout': 4,\n",
    "                'tier': 3\n",
    "            },\n",
    "            {\n",
    "                'name': 'Iconscout',\n",
    "                'url': 'https://cdn.iconscout.com/icon/{domain}',\n",
    "                'params': {},\n",
    "                'headers': {},\n",
    "                'timeout': 4,\n",
    "                'tier': 3\n",
    "            },\n",
    "            \n",
    "            # Tier 4: Social Media & Directory APIs\n",
    "            {\n",
    "                'name': 'Wikipedia',\n",
    "                'url': 'https://en.wikipedia.org/api/rest_v1/page/summary/{domain}',\n",
    "                'params': {},\n",
    "                'headers': {},\n",
    "                'timeout': 5,\n",
    "                'tier': 4\n",
    "            },\n",
    "            {\n",
    "                'name': 'Wikidata',\n",
    "                'url': 'https://www.wikidata.org/w/api.php',\n",
    "                'params': {'action': 'wbsearchentities', 'search': '{domain}', 'format': 'json', 'language': 'en'},\n",
    "                'headers': {},\n",
    "                'timeout': 5,\n",
    "                'tier': 4\n",
    "            },\n",
    "            {\n",
    "                'name': 'Company Logo DB',\n",
    "                'url': 'https://logo.clearbitjs.com/{domain}',\n",
    "                'params': {},\n",
    "                'headers': {},\n",
    "                'timeout': 4,\n",
    "                'tier': 4\n",
    "            },\n",
    "            {\n",
    "                'name': 'LogoTyp',\n",
    "                'url': 'https://logotyp.us/logo/{domain}',\n",
    "                'params': {},\n",
    "                'headers': {},\n",
    "                'timeout': 4,\n",
    "                'tier': 4\n",
    "            },\n",
    "            {\n",
    "                'name': 'OpenCorporates',\n",
    "                'url': 'https://api.opencorporates.com/companies/search',\n",
    "                'params': {'q': '{domain}', 'format': 'json'},\n",
    "                'headers': {},\n",
    "                'timeout': 5,\n",
    "                'tier': 4\n",
    "            },\n",
    "            \n",
    "            # Tier 5: Web Archive & Metadata\n",
    "            {\n",
    "                'name': 'Internet Archive',\n",
    "                'url': 'https://web.archive.org/cdx/search/cdx',\n",
    "                'params': {'url': '{domain}/favicon.ico', 'output': 'json', 'limit': '1'},\n",
    "                'headers': {},\n",
    "                'timeout': 6,\n",
    "                'tier': 5\n",
    "            },\n",
    "            {\n",
    "                'name': 'Archive Today',\n",
    "                'url': 'https://archive.today/timemap/json/{domain}',\n",
    "                'params': {},\n",
    "                'headers': {},\n",
    "                'timeout': 6,\n",
    "                'tier': 5\n",
    "            },\n",
    "            {\n",
    "                'name': 'Logo Garden',\n",
    "                'url': 'https://www.logoground.com/api/logo/{domain}',\n",
    "                'params': {},\n",
    "                'headers': {},\n",
    "                'timeout': 5,\n",
    "                'tier': 5\n",
    "            },\n",
    "            \n",
    "            # Tier 6: Direct Website Scraping (High success fallback)\n",
    "            {\n",
    "                'name': 'Direct Favicon',\n",
    "                'url': 'https://{domain}/favicon.ico',\n",
    "                'params': {},\n",
    "                'headers': {},\n",
    "                'timeout': 3,\n",
    "                'tier': 6\n",
    "            },\n",
    "            {\n",
    "                'name': 'Apple Touch Icon',\n",
    "                'url': 'https://{domain}/apple-touch-icon.png',\n",
    "                'params': {},\n",
    "                'headers': {},\n",
    "                'timeout': 3,\n",
    "                'tier': 6\n",
    "            },\n",
    "            {\n",
    "                'name': 'Apple Touch Icon 152',\n",
    "                'url': 'https://{domain}/apple-touch-icon-152x152.png',\n",
    "                'params': {},\n",
    "                'headers': {},\n",
    "                'timeout': 3,\n",
    "                'tier': 6\n",
    "            },\n",
    "            {\n",
    "                'name': 'Apple Touch Icon 180',\n",
    "                'url': 'https://{domain}/apple-touch-icon-180x180.png',\n",
    "                'params': {},\n",
    "                'headers': {},\n",
    "                'timeout': 3,\n",
    "                'tier': 6\n",
    "            },\n",
    "            {\n",
    "                'name': 'Android Chrome 192',\n",
    "                'url': 'https://{domain}/android-chrome-192x192.png',\n",
    "                'params': {},\n",
    "                'headers': {},\n",
    "                'timeout': 3,\n",
    "                'tier': 6\n",
    "            },\n",
    "            {\n",
    "                'name': 'Android Chrome 512',\n",
    "                'url': 'https://{domain}/android-chrome-512x512.png',\n",
    "                'params': {},\n",
    "                'headers': {},\n",
    "                'timeout': 3,\n",
    "                'tier': 6\n",
    "            },\n",
    "            {\n",
    "                'name': 'Site Logo PNG',\n",
    "                'url': 'https://{domain}/logo.png',\n",
    "                'params': {},\n",
    "                'headers': {},\n",
    "                'timeout': 3,\n",
    "                'tier': 6\n",
    "            },\n",
    "            {\n",
    "                'name': 'Site Logo SVG',\n",
    "                'url': 'https://{domain}/logo.svg',\n",
    "                'params': {},\n",
    "                'headers': {},\n",
    "                'timeout': 3,\n",
    "                'tier': 6\n",
    "            },\n",
    "            {\n",
    "                'name': 'Assets Logo',\n",
    "                'url': 'https://{domain}/assets/logo.png',\n",
    "                'params': {},\n",
    "                'headers': {},\n",
    "                'timeout': 3,\n",
    "                'tier': 6\n",
    "            },\n",
    "            {\n",
    "                'name': 'Images Logo',\n",
    "                'url': 'https://{domain}/images/logo.png',\n",
    "                'params': {},\n",
    "                'headers': {},\n",
    "                'timeout': 3,\n",
    "                'tier': 6\n",
    "            },\n",
    "            {\n",
    "                'name': 'Static Logo',\n",
    "                'url': 'https://{domain}/static/logo.png',\n",
    "                'params': {},\n",
    "                'headers': {},\n",
    "                'timeout': 3,\n",
    "                'tier': 6\n",
    "            },\n",
    "            {\n",
    "                'name': 'Brand Logo',\n",
    "                'url': 'https://{domain}/brand/logo.png',\n",
    "                'params': {},\n",
    "                'headers': {},\n",
    "                'timeout': 3,\n",
    "                'tier': 6\n",
    "            },\n",
    "            \n",
    "            # Tier 7: Alternative domains and variations  \n",
    "            {\n",
    "                'name': 'WWW Favicon',\n",
    "                'url': 'https://www.{domain}/favicon.ico',\n",
    "                'params': {},\n",
    "                'headers': {},\n",
    "                'timeout': 3,\n",
    "                'tier': 7\n",
    "            },\n",
    "            {\n",
    "                'name': 'WWW Logo',\n",
    "                'url': 'https://www.{domain}/logo.png',\n",
    "                'params': {},\n",
    "                'headers': {},\n",
    "                'timeout': 3,\n",
    "                'tier': 7\n",
    "            },\n",
    "            {\n",
    "                'name': 'CDN Logo',\n",
    "                'url': 'https://cdn.{domain}/logo.png',\n",
    "                'params': {},\n",
    "                'headers': {},\n",
    "                'timeout': 3,\n",
    "                'tier': 7\n",
    "            },\n",
    "            {\n",
    "                'name': 'Media Logo',\n",
    "                'url': 'https://media.{domain}/logo.png',\n",
    "                'params': {},\n",
    "                'headers': {},\n",
    "                'timeout': 3,\n",
    "                'tier': 7\n",
    "            },\n",
    "            \n",
    "            # Tier 8: DNS & WHOIS-Based Logo Discovery \n",
    "            {\n",
    "                'name': 'DNS-over-HTTPS Logo TXT',\n",
    "                'url': 'https://cloudflare-dns.com/dns-query',\n",
    "                'params': {'name': 'logo.{domain}', 'type': 'TXT', 'ct': 'application/dns-json'},\n",
    "                'headers': {'accept': 'application/dns-json'},\n",
    "                'timeout': 5,\n",
    "                'tier': 8,\n",
    "                'dns_query': True\n",
    "            },\n",
    "            {\n",
    "                'name': 'DNS-over-HTTPS Brand TXT',\n",
    "                'url': 'https://cloudflare-dns.com/dns-query',\n",
    "                'params': {'name': 'brand.{domain}', 'type': 'TXT', 'ct': 'application/dns-json'},\n",
    "                'headers': {'accept': 'application/dns-json'},\n",
    "                'timeout': 5,\n",
    "                'tier': 8,\n",
    "                'dns_query': True\n",
    "            },\n",
    "            {\n",
    "                'name': 'DNS-over-HTTPS Assets TXT',\n",
    "                'url': 'https://cloudflare-dns.com/dns-query',\n",
    "                'params': {'name': 'assets.{domain}', 'type': 'TXT', 'ct': 'application/dns-json'},\n",
    "                'headers': {'accept': 'application/dns-json'},\n",
    "                'timeout': 5,\n",
    "                'tier': 8,\n",
    "                'dns_query': True\n",
    "            },\n",
    "            {\n",
    "                'name': 'Google DNS Logo TXT',\n",
    "                'url': 'https://dns.google/resolve',\n",
    "                'params': {'name': 'logo.{domain}', 'type': 'TXT'},\n",
    "                'headers': {},\n",
    "                'timeout': 5,\n",
    "                'tier': 8,\n",
    "                'dns_query': True\n",
    "            },\n",
    "            {\n",
    "                'name': 'WHOIS Brand API',\n",
    "                'url': 'https://www.whoisxmlapi.com/whoisserver/WhoisService',\n",
    "                'params': {'domainName': '{domain}', 'outputFormat': 'JSON', 'apiKey': 'demo'},\n",
    "                'headers': {},\n",
    "                'timeout': 6,\n",
    "                'tier': 8,\n",
    "                'whois_query': True\n",
    "            },\n",
    "            {\n",
    "                'name': 'Domain Tools Logo',\n",
    "                'url': 'https://api.domaintools.com/v1/{domain}/hosting-history',\n",
    "                'params': {'format': 'json'},\n",
    "                'headers': {},\n",
    "                'timeout': 6,\n",
    "                'tier': 8,\n",
    "                'domain_meta': True\n",
    "            },\n",
    "            {\n",
    "                'name': 'SecurityTrails DNS',\n",
    "                'url': 'https://api.securitytrails.com/v1/domain/{domain}/subdomains',\n",
    "                'params': {},\n",
    "                'headers': {'APIKEY': 'demo'},\n",
    "                'timeout': 6,\n",
    "                'tier': 8,\n",
    "                'subdomain_scan': True\n",
    "            },\n",
    "            {\n",
    "                'name': 'VirusTotal Domain',\n",
    "                'url': 'https://www.virustotal.com/vtapi/v2/domain/report',\n",
    "                'params': {'domain': '{domain}', 'apikey': 'demo'},\n",
    "                'headers': {},\n",
    "                'timeout': 6,\n",
    "                'tier': 8,\n",
    "                'domain_intel': True\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    async def __aenter__(self):\n",
    "        timeout = aiohttp.ClientTimeout(total=20)  # Increased timeout for more APIs\n",
    "        connector = aiohttp.TCPConnector(limit=400, limit_per_host=150)  # Higher limits\n",
    "        self.session = aiohttp.ClientSession(\n",
    "            timeout=timeout,\n",
    "            connector=connector,\n",
    "            headers={'User-Agent': 'LogoMatcher/3.0 Ultra-Enhanced'}\n",
    "        )\n",
    "        return self\n",
    "    \n",
    "    async def __aexit__(self, exc_type, exc_val, exc_tb):\n",
    "        if self.session:\n",
    "            await self.session.close()\n",
    "    \n",
    "    def clean_domain(self, website: str) -> str:\n",
    "        \"\"\"Extract clean domain from website URL\"\"\"\n",
    "        if website.startswith(('http://', 'https://')):\n",
    "            from urllib.parse import urlparse\n",
    "            parsed = urlparse(website)\n",
    "            domain = parsed.netloc\n",
    "            # Remove www. prefix for cleaner API calls\n",
    "            if domain.startswith('www.'):\n",
    "                domain = domain[4:]\n",
    "            return domain\n",
    "        return website\n",
    "    \n",
    "    async def try_api_service(self, api_config: dict, domain: str) -> Optional[Dict]:\n",
    "        \"\"\"Try a single API service for logo\"\"\"\n",
    "        try:\n",
    "            # Format URL\n",
    "            if '{domain}' in api_config['url']:\n",
    "                url = api_config['url'].format(domain=domain)\n",
    "            else:\n",
    "                url = api_config['url']\n",
    "            \n",
    "            # Format params\n",
    "            params = {}\n",
    "            for key, value in api_config.get('params', {}).items():\n",
    "                if '{domain}' in str(value):\n",
    "                    params[key] = value.format(domain=domain)\n",
    "                else:\n",
    "                    params[key] = value\n",
    "            \n",
    "            # Make request\n",
    "            timeout = aiohttp.ClientTimeout(total=api_config['timeout'])\n",
    "            async with self.session.get(\n",
    "                url, \n",
    "                params=params,\n",
    "                headers=api_config.get('headers', {}),\n",
    "                timeout=timeout,\n",
    "                allow_redirects=True  # Follow redirects for better coverage\n",
    "            ) as response:\n",
    "                \n",
    "                if response.status == 200:\n",
    "                    content_type = response.headers.get('content-type', '')\n",
    "                    \n",
    "                    # Handle different response types\n",
    "                    if 'image' in content_type:\n",
    "                        content = await response.read()\n",
    "                        if len(content) > 200:  # Lowered threshold for more logos\n",
    "                            return {\n",
    "                                'data': content,\n",
    "                                'url': str(response.url),\n",
    "                                'content_type': content_type,\n",
    "                                'size': len(content)\n",
    "                            }\n",
    "                    \n",
    "                    elif 'json' in content_type or api_config.get('dns_query') or api_config.get('whois_query'):\n",
    "                        # Handle JSON responses (Wikipedia, Wikidata, DNS, WHOIS, etc.)\n",
    "                        json_data = await response.json()\n",
    "                        logo_url = self.extract_logo_from_json(json_data, api_config['name'])\n",
    "                        if logo_url:\n",
    "                            # Download the actual logo\n",
    "                            logo_result = await self.download_logo_from_url(logo_url)\n",
    "                            if logo_result:\n",
    "                                return logo_result\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Silent fail for speed - but we can uncomment for debugging\n",
    "            # print(f\"API {api_config['name']} failed for {domain}: {e}\")\n",
    "            pass\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def extract_logo_from_json(self, json_data: dict, api_name: str) -> Optional[str]:\n",
    "        \"\"\"Extract logo URL from JSON API responses\"\"\"\n",
    "        try:\n",
    "            if api_name == 'Wikipedia':\n",
    "                if 'thumbnail' in json_data and 'source' in json_data['thumbnail']:\n",
    "                    return json_data['thumbnail']['source']\n",
    "                elif 'originalimage' in json_data and 'source' in json_data['originalimage']:\n",
    "                    return json_data['originalimage']['source']\n",
    "            \n",
    "            elif api_name == 'Wikidata':\n",
    "                if 'search' in json_data and json_data['search']:\n",
    "                    for item in json_data['search']:\n",
    "                        if 'display' in item and 'label' in item['display']:\n",
    "                            # This would need additional API calls to get the actual logo\n",
    "                            pass\n",
    "            \n",
    "            elif api_name == 'Favicon Grabber':\n",
    "                if 'icons' in json_data and json_data['icons']:\n",
    "                    # Return the largest icon\n",
    "                    largest_icon = max(json_data['icons'], key=lambda x: x.get('sizes', '0x0').split('x')[0])\n",
    "                    return largest_icon.get('src')\n",
    "            \n",
    "            elif api_name == 'OpenCorporates':\n",
    "                if 'results' in json_data and json_data['results']:\n",
    "                    for company in json_data['results']['companies']:\n",
    "                        if 'company' in company and 'registry_url' in company['company']:\n",
    "                            # Additional processing could extract logos from company pages\n",
    "                            pass\n",
    "            \n",
    "            # DNS-based Logo Discovery\n",
    "            elif 'DNS Logo TXT' in api_name or 'DNS Brand TXT' in api_name or 'DNS Assets TXT' in api_name:\n",
    "                # Parse DNS TXT records for logo URLs\n",
    "                if 'Answer' in json_data:\n",
    "                    for record in json_data['Answer']:\n",
    "                        if record.get('type') == 16:  # TXT record\n",
    "                            txt_data = record.get('data', '')\n",
    "                            # Look for logo URLs in TXT records\n",
    "                            logo_url = self.extract_logo_url_from_txt(txt_data)\n",
    "                            if logo_url:\n",
    "                                return logo_url\n",
    "                elif 'answer' in json_data:  # Google DNS format\n",
    "                    for record in json_data['answer']:\n",
    "                        if record.get('type') == 16:\n",
    "                            txt_data = record.get('data', '')\n",
    "                            logo_url = self.extract_logo_url_from_txt(txt_data)\n",
    "                            if logo_url:\n",
    "                                return logo_url\n",
    "            \n",
    "            elif api_name == 'WHOIS Brand API':\n",
    "                # Extract logo info from WHOIS data\n",
    "                whois_data = json_data.get('WhoisRecord', {})\n",
    "                registrant = whois_data.get('registrant', {})\n",
    "                if 'organization' in registrant:\n",
    "                    # Could cross-reference with other APIs\n",
    "                    pass\n",
    "            \n",
    "            elif api_name == 'SecurityTrails DNS':\n",
    "                # Look for logo-related subdomains\n",
    "                if 'subdomains' in json_data:\n",
    "                    for subdomain in json_data['subdomains']:\n",
    "                        if any(keyword in subdomain.lower() for keyword in ['logo', 'brand', 'assets', 'cdn', 'static']):\n",
    "                            # Try common logo paths on these subdomains\n",
    "                            potential_url = f\"https://{subdomain}.{json_data.get('domain', '')}/logo.png\"\n",
    "                            return potential_url\n",
    "                        \n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def extract_logo_url_from_txt(self, txt_data: str) -> Optional[str]:\n",
    "        \"\"\"Extract logo URL from DNS TXT record data\"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Common TXT record patterns for logo URLs\n",
    "        patterns = [\n",
    "            r'logo[_-]?url[=:]\\s*([^\\s\"\\']+)',  # logo_url=https://...\n",
    "            r'brand[_-]?logo[=:]\\s*([^\\s\"\\']+)',  # brand_logo=https://...\n",
    "            r'icon[_-]?url[=:]\\s*([^\\s\"\\']+)',   # icon_url=https://...\n",
    "            r'(https?://[^\\s\"\\']+\\.(?:png|jpg|jpeg|svg|gif|webp))',  # Direct URL patterns\n",
    "            r'assets[=:]\\s*([^\\s\"\\']+)',  # assets=https://cdn.../logo.png\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, txt_data, re.IGNORECASE)\n",
    "            if match:\n",
    "                url = match.group(1)\n",
    "                if url.startswith(('http://', 'https://')):\n",
    "                    return url\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    async def download_logo_from_url(self, logo_url: str) -> Optional[Dict]:\n",
    "        \"\"\"Download logo from extracted URL\"\"\"\n",
    "        try:\n",
    "            timeout = aiohttp.ClientTimeout(total=5)\n",
    "            async with self.session.get(logo_url, timeout=timeout, allow_redirects=True) as response:\n",
    "                if response.status == 200:\n",
    "                    content_type = response.headers.get('content-type', '')\n",
    "                    if 'image' in content_type:\n",
    "                        content = await response.read()\n",
    "                        if len(content) > 200:\n",
    "                            return {\n",
    "                                'data': content,\n",
    "                                'url': logo_url,\n",
    "                                'content_type': content_type,\n",
    "                                'size': len(content)\n",
    "                            }\n",
    "        except Exception:\n",
    "            pass\n",
    "        return None\n",
    "    \n",
    "    async def extract_logo_tiered(self, website: str, max_tier: int = 8) -> Dict:\n",
    "        \"\"\"Extract logo using expanded tiered API approach for 97%+ success\"\"\"\n",
    "        domain = self.clean_domain(website)\n",
    "        \n",
    "        result = {\n",
    "            'website': website,\n",
    "            'domain': domain,\n",
    "            'logo_found': False,\n",
    "            'logo_url': None,\n",
    "            'logo_data': None,\n",
    "            'method': 'ultra_enhanced_api',\n",
    "            'api_service': None,\n",
    "            'tier_used': None,\n",
    "            'attempts': 0,\n",
    "            'error': None\n",
    "        }\n",
    "        \n",
    "        # Try APIs by tier for maximum efficiency\n",
    "        for tier in range(1, max_tier + 1):\n",
    "            tier_apis = [api for api in self.logo_apis if api.get('tier') == tier]\n",
    "            \n",
    "            # Try all APIs in current tier concurrently\n",
    "            if tier_apis:\n",
    "                tasks = [self.try_api_service(api_config, domain) for api_config in tier_apis]\n",
    "                tier_results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "                \n",
    "                # Check for success in this tier\n",
    "                for i, logo_result in enumerate(tier_results):\n",
    "                    if isinstance(logo_result, dict) and logo_result:\n",
    "                        result.update({\n",
    "                            'logo_found': True,\n",
    "                            'logo_url': logo_result['url'],\n",
    "                            'logo_data': logo_result['data'],\n",
    "                            'method': 'ultra_enhanced_api',\n",
    "                            'api_service': tier_apis[i]['name'],\n",
    "                            'tier_used': tier,\n",
    "                            'attempts': result['attempts'] + len(tier_apis)\n",
    "                        })\n",
    "                        return result\n",
    "                \n",
    "                result['attempts'] += len(tier_apis)\n",
    "                \n",
    "                # Brief pause between tiers (less for early tiers)\n",
    "                if tier <= 4:\n",
    "                    await asyncio.sleep(0.1)\n",
    "                else:\n",
    "                    await asyncio.sleep(0.2)  # Longer pause for slower tiers\n",
    "        \n",
    "        result['error'] = f'All {result[\"attempts\"]} APIs failed'\n",
    "        return result\n",
    "    \n",
    "    async def extract_logo_exhaustive_retry(self, website: str, max_tier: int = 7) -> Dict:\n",
    "        \"\"\"\n",
    "        EXHAUSTIVE RETRY: Try failed websites against ALL APIs in random order\n",
    "        This maximizes success rate by trying different API combinations\n",
    "        \"\"\"\n",
    "        domain = self.clean_domain(website)\n",
    "        \n",
    "        result = {\n",
    "            'website': website,\n",
    "            'domain': domain,\n",
    "            'logo_found': False,\n",
    "            'logo_url': None,\n",
    "            'logo_data': None,\n",
    "            'method': 'exhaustive_retry',\n",
    "            'api_service': None,\n",
    "            'tier_used': None,\n",
    "            'attempts': 0,\n",
    "            'error': None\n",
    "        }\n",
    "        \n",
    "        # Get ALL APIs up to max_tier and shuffle them for random order\n",
    "        import random\n",
    "        all_apis = [api for api in self.logo_apis if api.get('tier', 1) <= max_tier]\n",
    "        random.shuffle(all_apis)  # Random order for better coverage\n",
    "        \n",
    "        print(f\"Exhaustive retry for {domain}: trying {len(all_apis)} APIs\")\n",
    "        \n",
    "        # Try APIs in smaller chunks to be respectful\n",
    "        chunk_size = 5\n",
    "        for i in range(0, len(all_apis), chunk_size):\n",
    "            chunk = all_apis[i:i + chunk_size]\n",
    "            \n",
    "            # Try chunk concurrently\n",
    "            tasks = [self.try_api_service(api_config, domain) for api_config in chunk]\n",
    "            chunk_results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            \n",
    "            # Check for success in this chunk\n",
    "            for j, logo_result in enumerate(chunk_results):\n",
    "                if isinstance(logo_result, dict) and logo_result:\n",
    "                    result.update({\n",
    "                        'logo_found': True,\n",
    "                        'logo_url': logo_result['url'],\n",
    "                        'logo_data': logo_result['data'],\n",
    "                        'method': 'exhaustive_retry',\n",
    "                        'api_service': chunk[j]['name'],\n",
    "                        'tier_used': chunk[j]['tier'],\n",
    "                        'attempts': result['attempts'] + len(chunk)\n",
    "                    })\n",
    "                    print(f\"Retry success for {domain}: {chunk[j]['name']}\")\n",
    "                    return result\n",
    "            \n",
    "            result['attempts'] += len(chunk)\n",
    "            \n",
    "            # Brief pause between chunks\n",
    "            await asyncio.sleep(0.1)\n",
    "        \n",
    "        result['error'] = f'Exhaustive retry failed: {result[\"attempts\"]} APIs tried'\n",
    "        return result\n",
    "    \n",
    "    async def batch_extract_logos_enhanced(self, websites: List[str], max_tier: int = 8) -> List[Dict]:\n",
    "        print(f\"ULTRA-ENHANCED API extraction: {len(websites)} websites\")\n",
    "        print(f\"Using {len([api for api in self.logo_apis if api.get('tier', 1) <= max_tier])} APIs across {max_tier} tiers\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Process websites in optimal batch size\n",
    "        batch_size = 30  # Smaller batches for more APIs\n",
    "        all_results = []\n",
    "        \n",
    "        for i in range(0, len(websites), batch_size):\n",
    "            batch = websites[i:i + batch_size]\n",
    "            batch_num = i//batch_size + 1\n",
    "            total_batches = (len(websites)-1)//batch_size + 1\n",
    "            \n",
    "            print(f\"   Batch {batch_num}/{total_batches}: {len(batch)} websites\")\n",
    "            \n",
    "            # Process batch concurrently\n",
    "            tasks = [self.extract_logo_tiered(website, max_tier) for website in batch]\n",
    "            batch_results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            \n",
    "            # Filter results\n",
    "            for j, result in enumerate(batch_results):\n",
    "                if isinstance(result, dict):\n",
    "                    all_results.append(result)\n",
    "                else:\n",
    "                    all_results.append({\n",
    "                        'website': batch[j],\n",
    "                        'logo_found': False,\n",
    "                        'error': f'Exception: {type(result).__name__}'\n",
    "                    })\n",
    "            \n",
    "            # Show batch progress\n",
    "            batch_successful = sum(1 for r in batch_results if isinstance(r, dict) and r.get('logo_found', False))\n",
    "            print(f\"Batch success: {batch_successful}/{len(batch)} ({batch_successful/len(batch)*100:.1f}%)\")\n",
    "            \n",
    "            # Brief pause between batches\n",
    "            await asyncio.sleep(0.3)\n",
    "        \n",
    "        # EXHAUSTIVE RETRY for failed websites\n",
    "        failed_results = [r for r in all_results if not r['logo_found']]\n",
    "        if failed_results and len(failed_results) <= 50:  # Only retry if not too many failures\n",
    "            print(f\"\\nEXHAUSTIVE RETRY PHASE\")\n",
    "            print(f\"Retrying {len(failed_results)} failed websites with ALL APIs...\")\n",
    "            \n",
    "            retry_websites = [r['website'] for r in failed_results]\n",
    "            retry_tasks = [self.extract_logo_exhaustive_retry(website, max_tier) for website in retry_websites]\n",
    "            retry_results = await asyncio.gather(*retry_tasks, return_exceptions=True)\n",
    "            \n",
    "            # Update original results with retry successes\n",
    "            retry_successes = 0\n",
    "            for i, retry_result in enumerate(retry_results):\n",
    "                if isinstance(retry_result, dict) and retry_result.get('logo_found', False):\n",
    "                    # Find and update the original failed result\n",
    "                    original_website = retry_websites[i]\n",
    "                    for j, original_result in enumerate(all_results):\n",
    "                        if original_result['website'] == original_website and not original_result['logo_found']:\n",
    "                            all_results[j] = retry_result\n",
    "                            retry_successes += 1\n",
    "                            break\n",
    "            \n",
    "            if retry_successes > 0:\n",
    "                print(f\"Exhaustive retry recovered {retry_successes} additional logos!\")\n",
    "            else:\n",
    "                print(\"No additional logos found in retry phase\")\n",
    "        \n",
    "        elif len(failed_results) > 50:\n",
    "            print(f\"\\nSkipping exhaustive retry: {len(failed_results)} failures (too many)\")\n",
    "            print(\"Consider increasing max_tier or checking network connectivity\")\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        successful = sum(1 for r in all_results if r['logo_found'])\n",
    "        success_rate = successful / len(websites) * 100\n",
    "        \n",
    "        print(f\"ULTRA-ENHANCED results: {successful}/{len(websites)} in {elapsed:.1f}s\")\n",
    "        print(f\"SUCCESS RATE: {success_rate:.1f}%\")\n",
    "        print(f\"Speed: {len(websites)/elapsed:.1f} websites/second\")\n",
    "        \n",
    "        # Show comprehensive breakdown\n",
    "        tier_breakdown = defaultdict(int)\n",
    "        api_breakdown = defaultdict(int)\n",
    "        \n",
    "        for result in all_results:\n",
    "            if result['logo_found']:\n",
    "                tier = result.get('tier_used', 'unknown')\n",
    "                service = result.get('api_service', 'unknown')\n",
    "                tier_breakdown[f\"Tier {tier}\"] += 1\n",
    "                api_breakdown[service] += 1\n",
    "        \n",
    "        print(\"\\nPERFORMANCE BREAKDOWN:\")\n",
    "        print(\"By Tier:\")\n",
    "        for tier, count in sorted(tier_breakdown.items()):\n",
    "            percentage = count / successful * 100 if successful > 0 else 0\n",
    "            print(f\"   - {tier}: {count} logos ({percentage:.1f}%)\")\n",
    "        \n",
    "        print(\"Top API Services:\")\n",
    "        for service, count in sorted(api_breakdown.items(), key=lambda x: x[1], reverse=True)[:8]:\n",
    "            percentage = count / successful * 100 if successful > 0 else 0\n",
    "            print(f\"   - {service}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Success rate assessment\n",
    "        if success_rate >= 97:\n",
    "            print(f\"EXCELLENT! {success_rate:.1f}% SUCCESS RATE ACHIEVED!\")\n",
    "        elif success_rate >= 95:\n",
    "            print(f\"\\nVERY GOOD! {success_rate:.1f}% success rate\")\n",
    "            print(\"Close to 97% target - consider adding tier 8 for remaining sites\")\n",
    "        elif success_rate >= 90:\n",
    "            print(f\"\\nGOOD! {success_rate:.1f}% success rate\")\n",
    "            print(\"To reach 97%+: increase max_tier or add more API services\")\n",
    "        else:\n",
    "            print(f\"\\n{success_rate:.1f}% success rate - needs improvement\")\n",
    "            print(\"Try max_tier=7 and check API service availability\")\n",
    "        \n",
    "        return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19e25ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UnionFind:\n",
    "    \"\"\"Union-Find data structure for clustering\"\"\"\n",
    "    \n",
    "    def __init__(self, elements):\n",
    "        self.parent = {elem: elem for elem in elements}\n",
    "        self.rank = {elem: 0 for elem in elements}\n",
    "    \n",
    "    def find(self, x):\n",
    "        if self.parent[x] != x:\n",
    "            self.parent[x] = self.find(self.parent[x])  # Path compression\n",
    "        return self.parent[x]\n",
    "    \n",
    "    def union(self, x, y):\n",
    "        px, py = self.find(x), self.find(y)\n",
    "        if px == py:\n",
    "            return\n",
    "        \n",
    "        # Union by rank\n",
    "        if self.rank[px] < self.rank[py]:\n",
    "            px, py = py, px\n",
    "        self.parent[py] = px\n",
    "        if self.rank[px] == self.rank[py]:\n",
    "            self.rank[px] += 1\n",
    "    \n",
    "    def get_clusters(self):\n",
    "        clusters = defaultdict(list)\n",
    "        for elem in self.parent:\n",
    "            root = self.find(elem)\n",
    "            clusters[root].append(elem)\n",
    "        return [cluster for cluster in clusters.values() if len(cluster) > 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc82de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "class FourierLogoAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.similarity_threshold_phash = 6  # Hamming distance\n",
    "        self.similarity_threshold_fft = 0.985  # Cosine similarity\n",
    "        self.similarity_threshold_fmt = 0.995  # Fourier-Mellin\n",
    "    \n",
    "    def compute_phash(self, img: np.ndarray) -> str:\n",
    "        \"\"\"Compute perceptual hash using DCT (Fourier cousin)\"\"\"\n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Resize to 32x32 for DCT\n",
    "        resized = cv2.resize(gray, (32, 32))\n",
    "        \n",
    "        # Compute DCT (like 2D Fourier but with cosines)\n",
    "        dct = cv2.dct(np.float32(resized))\n",
    "        \n",
    "        # Take top-left 8x8 (low frequencies)\n",
    "        dct_low = dct[0:8, 0:8]\n",
    "        \n",
    "        # Compare with median to create binary hash\n",
    "        median = np.median(dct_low)\n",
    "        binary = dct_low > median\n",
    "        \n",
    "        # Convert to hex string\n",
    "        hash_str = ''.join(['1' if b else '0' for b in binary.flatten()])\n",
    "        return hash_str\n",
    "    \n",
    "    def hamming_distance(self, hash1: str, hash2: str) -> int:\n",
    "        \"\"\"Calculate Hamming distance between two hashes\"\"\"\n",
    "        return sum(c1 != c2 for c1, c2 in zip(hash1, hash2))\n",
    "    \n",
    "    def compute_fft_features(self, img: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute FFT low-frequency features for global shape\"\"\"\n",
    "        # Convert to grayscale and normalize\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        gray = gray.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Resize to square and standard size\n",
    "        size = 128\n",
    "        resized = cv2.resize(gray, (size, size))\n",
    "        \n",
    "        # Compute 2D FFT\n",
    "        fft = fft2(resized)\n",
    "        fft_shifted = fftshift(fft)\n",
    "        \n",
    "        # Take magnitude and apply log\n",
    "        magnitude = np.abs(fft_shifted)\n",
    "        log_magnitude = np.log(magnitude + 1e-8)\n",
    "        \n",
    "        # Extract central low-frequency block (32x32)\n",
    "        center = size // 2\n",
    "        crop_size = 16\n",
    "        low_freq = log_magnitude[\n",
    "            center-crop_size:center+crop_size,\n",
    "            center-crop_size:center+crop_size\n",
    "        ]\n",
    "        \n",
    "        # Flatten and normalize\n",
    "        features = low_freq.flatten()\n",
    "        features = features / (np.linalg.norm(features) + 1e-8)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def compute_fourier_mellin_signature(self, img: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute Fourier-Mellin theta signature for rotation/scale invariance\"\"\"\n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        gray = gray.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Resize to square\n",
    "        size = 128\n",
    "        resized = cv2.resize(gray, (size, size))\n",
    "        \n",
    "        # Compute FFT and get magnitude\n",
    "        fft = fft2(resized)\n",
    "        fft_shifted = fftshift(fft)\n",
    "        magnitude = np.abs(fft_shifted)\n",
    "        \n",
    "        # Convert to log-polar coordinates\n",
    "        center = size // 2\n",
    "        theta_samples = 64\n",
    "        radius_samples = 32\n",
    "        \n",
    "        # Create theta signature by averaging over radius\n",
    "        theta_signature = np.zeros(theta_samples)\n",
    "        \n",
    "        for i, theta in enumerate(np.linspace(0, 2*np.pi, theta_samples, endpoint=False)):\n",
    "            # Sample along radial lines\n",
    "            radial_sum = 0\n",
    "            for r in np.linspace(1, center-1, radius_samples):\n",
    "                x = int(center + r * np.cos(theta))\n",
    "                y = int(center + r * np.sin(theta))\n",
    "                if 0 <= x < size and 0 <= y < size:\n",
    "                    radial_sum += magnitude[y, x]\n",
    "            theta_signature[i] = radial_sum\n",
    "        \n",
    "        # Normalize\n",
    "        theta_signature = theta_signature / (np.linalg.norm(theta_signature) + 1e-8)\n",
    "        \n",
    "        return theta_signature\n",
    "    \n",
    "    def compare_fourier_mellin(self, sig1: np.ndarray, sig2: np.ndarray) -> float:\n",
    "        \"\"\"Compare Fourier-Mellin signatures with rotation invariance\"\"\"\n",
    "        # Use FFT to efficiently compute circular correlation\n",
    "        # This finds the best alignment over all rotations\n",
    "        n = len(sig1)\n",
    "        \n",
    "        # Pad and compute correlation via FFT\n",
    "        sig1_fft = np.fft.rfft(sig1, n=2*n)\n",
    "        sig2_fft = np.fft.rfft(sig2[::-1], n=2*n)  # Reverse for correlation\n",
    "        \n",
    "        correlation = np.fft.irfft(sig1_fft * sig2_fft)\n",
    "        \n",
    "        # Find maximum correlation (best rotation alignment)\n",
    "        max_correlation = np.max(correlation)\n",
    "        \n",
    "        return max_correlation\n",
    "    \n",
    "    def compute_all_features(self, img: np.ndarray) -> Dict:\n",
    "        \"\"\"Compute all features including Fourier and keypoint-based methods\"\"\"\n",
    "        # Fourier-based features\n",
    "        fourier_features = {\n",
    "            'phash': self.compute_phash(img),\n",
    "            'fft_features': self.compute_fft_features(img),\n",
    "            'fmt_signature': self.compute_fourier_mellin_signature(img)\n",
    "        }\n",
    "        \n",
    "        # Keypoint-based features\n",
    "        sift_features = self.compute_sift_features(img)\n",
    "        orb_features = self.compute_orb_features(img)\n",
    "        \n",
    "        return {\n",
    "            **fourier_features,\n",
    "            'sift': sift_features,\n",
    "            'orb': orb_features\n",
    "        }\n",
    "    \n",
    "    def compute_sift_features(self, img: np.ndarray) -> Dict:\n",
    "        \"\"\"Compute SIFT keypoints and descriptors for logo matching\"\"\"\n",
    "        try:\n",
    "            # Initialize SIFT detector\n",
    "            sift = cv2.SIFT_create(nfeatures=100)  # Limit keypoints for logos\n",
    "            \n",
    "            # Detect keypoints and compute descriptors\n",
    "            keypoints, descriptors = sift.detectAndCompute(img, None)\n",
    "            \n",
    "            if descriptors is None or len(descriptors) == 0:\n",
    "                return {'valid': False, 'keypoints': [], 'descriptors': np.array([]), 'signature': np.zeros(256)}\n",
    "            \n",
    "            # Create compact feature representation\n",
    "            # Use mean and std of descriptors as global signature\n",
    "            desc_mean = np.mean(descriptors, axis=0) if len(descriptors) > 0 else np.zeros(128)\n",
    "            desc_std = np.std(descriptors, axis=0) if len(descriptors) > 0 else np.zeros(128)\n",
    "            \n",
    "            return {\n",
    "                'valid': True,\n",
    "                'keypoint_count': len(keypoints),\n",
    "                'descriptors': descriptors,\n",
    "                'signature': np.concatenate([desc_mean, desc_std])  # 256-dim signature\n",
    "            }\n",
    "            \n",
    "        except Exception:\n",
    "            return {'valid': False, 'keypoints': [], 'descriptors': np.array([]), 'signature': np.zeros(256)}\n",
    "    \n",
    "    def compute_orb_features(self, img: np.ndarray) -> Dict:\n",
    "        \"\"\"Compute ORB keypoints and descriptors (faster alternative to SIFT)\"\"\"\n",
    "        try:\n",
    "            # Initialize ORB detector\n",
    "            orb = cv2.ORB_create(nfeatures=50)  # Fewer features for speed\n",
    "            \n",
    "            # Detect keypoints and compute descriptors\n",
    "            keypoints, descriptors = orb.detectAndCompute(img, None)\n",
    "            \n",
    "            if descriptors is None or len(descriptors) == 0:\n",
    "                return {'valid': False, 'keypoints': [], 'descriptors': np.array([]), 'signature': np.zeros(32)}\n",
    "            \n",
    "            # ORB descriptors are binary, create signature differently\n",
    "            desc_mean = np.mean(descriptors.astype(np.float32), axis=0) if len(descriptors) > 0 else np.zeros(32)\n",
    "            \n",
    "            return {\n",
    "                'valid': True,\n",
    "                'keypoint_count': len(keypoints),\n",
    "                'descriptors': descriptors,\n",
    "                'signature': desc_mean  # 32-dim signature\n",
    "            }\n",
    "            \n",
    "        except Exception:\n",
    "            return {'valid': False, 'keypoints': [], 'descriptors': np.array([]), 'signature': np.zeros(32)}\n",
    "    \n",
    "    def match_sift_features(self, desc1: np.ndarray, desc2: np.ndarray) -> float:\n",
    "        \"\"\"Match SIFT descriptors using FLANN matcher\"\"\"\n",
    "        try:\n",
    "            if len(desc1) == 0 or len(desc2) == 0:\n",
    "                return 0.0\n",
    "            \n",
    "            # FLANN parameters for SIFT\n",
    "            FLANN_INDEX_KDTREE = 1\n",
    "            index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "            search_params = dict(checks=50)\n",
    "            \n",
    "            flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "            matches = flann.knnMatch(desc1, desc2, k=2)\n",
    "            \n",
    "            # Apply Lowe's ratio test\n",
    "            good_matches = []\n",
    "            for match_pair in matches:\n",
    "                if len(match_pair) == 2:\n",
    "                    m, n = match_pair\n",
    "                    if m.distance < 0.7 * n.distance:  # Lowe's ratio\n",
    "                        good_matches.append(m)\n",
    "            \n",
    "            # Return ratio of good matches to total possible matches\n",
    "            total_features = min(len(desc1), len(desc2))\n",
    "            return len(good_matches) / max(total_features, 1)\n",
    "            \n",
    "        except Exception:\n",
    "            return 0.0\n",
    "    \n",
    "    def match_orb_features(self, desc1: np.ndarray, desc2: np.ndarray) -> float:\n",
    "        \"\"\"Match ORB descriptors using Hamming distance\"\"\"\n",
    "        try:\n",
    "            if len(desc1) == 0 or len(desc2) == 0:\n",
    "                return 0.0\n",
    "            \n",
    "            # Use BFMatcher for binary descriptors (ORB)\n",
    "            bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "            matches = bf.match(desc1, desc2)\n",
    "            \n",
    "            # Filter good matches based on distance\n",
    "            good_matches = [m for m in matches if m.distance < 50]  # Hamming distance threshold\n",
    "            \n",
    "            # Return ratio of good matches\n",
    "            total_features = min(len(desc1), len(desc2))\n",
    "            return len(good_matches) / max(total_features, 1)\n",
    "            \n",
    "        except Exception:\n",
    "            return 0.0\n",
    "    \n",
    "    def are_similar(self, features1: Dict, features2: Dict) -> Tuple[bool, Dict]:\n",
    "        \"\"\"Determine if two logos are similar using multiple Fourier methods\"\"\"\n",
    "        # pHash comparison (Hamming distance)\n",
    "        phash_distance = self.hamming_distance(features1['phash'], features2['phash'])\n",
    "        phash_similar = phash_distance <= self.similarity_threshold_phash\n",
    "        \n",
    "        # FFT features comparison (cosine similarity)\n",
    "        fft_similarity = cosine_similarity(\n",
    "            features1['fft_features'].reshape(1, -1),\n",
    "            features2['fft_features'].reshape(1, -1)\n",
    "        )[0, 0]\n",
    "        fft_similar = fft_similarity >= self.similarity_threshold_fft\n",
    "        \n",
    "        # Fourier-Mellin comparison (rotation/scale invariant)\n",
    "        fmt_similarity = self.compare_fourier_mellin(\n",
    "            features1['fmt_signature'],\n",
    "            features2['fmt_signature']\n",
    "        )\n",
    "        fmt_similar = fmt_similarity >= self.similarity_threshold_fmt\n",
    "        \n",
    "        # SIFT keypoint matching\n",
    "        sift_similarity = 0.0\n",
    "        sift_similar = False\n",
    "        if features1['sift']['valid'] and features2['sift']['valid']:\n",
    "            # Try descriptor matching\n",
    "            sift_similarity = self.match_sift_features(\n",
    "                features1['sift']['descriptors'], \n",
    "                features2['sift']['descriptors']\n",
    "            )\n",
    "            # Also compare signature vectors\n",
    "            if len(features1['sift']['signature']) > 0 and len(features2['sift']['signature']) > 0:\n",
    "                sift_sig_similarity = cosine_similarity(\n",
    "                    features1['sift']['signature'].reshape(1, -1),\n",
    "                    features2['sift']['signature'].reshape(1, -1)\n",
    "                )[0, 0]\n",
    "                sift_similarity = max(sift_similarity, sift_sig_similarity)\n",
    "            sift_similar = sift_similarity >= 0.3  # SIFT threshold\n",
    "        \n",
    "        # ORB keypoint matching  \n",
    "        orb_similarity = 0.0\n",
    "        orb_similar = False\n",
    "        if features1['orb']['valid'] and features2['orb']['valid']:\n",
    "            # Try descriptor matching\n",
    "            orb_similarity = self.match_orb_features(\n",
    "                features1['orb']['descriptors'], \n",
    "                features2['orb']['descriptors']\n",
    "            )\n",
    "            # Also compare signature vectors\n",
    "            if len(features1['orb']['signature']) > 0 and len(features2['orb']['signature']) > 0:\n",
    "                orb_sig_similarity = cosine_similarity(\n",
    "                    features1['orb']['signature'].reshape(1, -1),\n",
    "                    features2['orb']['signature'].reshape(1, -1)\n",
    "                )[0, 0]\n",
    "                orb_similarity = max(orb_similarity, orb_sig_similarity)\n",
    "            orb_similar = orb_similarity >= 0.25  # ORB threshold\n",
    "        \n",
    "        # Enhanced combination: Fourier OR keypoint methods\n",
    "        is_similar = phash_similar or fft_similar or fmt_similar or sift_similar or orb_similar\n",
    "        \n",
    "        metrics = {\n",
    "            'phash_distance': phash_distance,\n",
    "            'phash_similar': phash_similar,\n",
    "            'fft_similarity': fft_similarity,\n",
    "            'fft_similar': fft_similar,\n",
    "            'fmt_similarity': fmt_similarity,\n",
    "            'fmt_similar': fmt_similar,\n",
    "            'sift_similarity': sift_similarity,\n",
    "            'sift_similar': sift_similar,\n",
    "            'orb_similarity': orb_similarity,\n",
    "            'orb_similar': orb_similar,\n",
    "            'overall_similar': is_similar\n",
    "        }\n",
    "        \n",
    "        return is_similar, metrics\n",
    "    \n",
    "    def preprocess_logo(self, logo_data: bytes) -> Optional[np.ndarray]:\n",
    "        \"\"\"Convert logo bytes to numpy array\"\"\"\n",
    "        try:\n",
    "            # Convert bytes to PIL Image\n",
    "            image = Image.open(io.BytesIO(logo_data))\n",
    "            \n",
    "            # Convert to RGB if needed\n",
    "            if image.mode != 'RGB':\n",
    "                image = image.convert('RGB')\n",
    "            \n",
    "            # Resize to standard size\n",
    "            image = image.resize((128, 128), Image.Resampling.LANCZOS)\n",
    "            \n",
    "            # Convert to numpy array (RGB format)\n",
    "            img_array = np.array(image)\n",
    "            \n",
    "            # Convert RGB to BGR for OpenCV compatibility\n",
    "            img_bgr = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            return img_bgr\n",
    "        except Exception as e:\n",
    "            return None\n",
    "    \n",
    "    def analyze_logo_batch(self, logos: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Analyze a batch of logos and extract Fourier features\"\"\"\n",
    "        print(f\"Analyzing {len(logos)} logos for Fourier features...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        analyzed_logos = []\n",
    "        successful_analysis = 0\n",
    "        \n",
    "        for i, logo in enumerate(logos):\n",
    "            if i % 100 == 0 and i > 0:\n",
    "                print(f\"   Progress: {i}/{len(logos)}\")\n",
    "            \n",
    "            try:\n",
    "                # Preprocess logo data\n",
    "                img = self.preprocess_logo(logo['logo_data'])\n",
    "                \n",
    "                if img is not None:\n",
    "                    # Extract all Fourier features\n",
    "                    features = self.compute_all_features(img)\n",
    "                    features['valid'] = True\n",
    "                    successful_analysis += 1\n",
    "                else:\n",
    "                    features = {'valid': False}\n",
    "                \n",
    "                # Add features to logo data\n",
    "                logo_with_features = logo.copy()\n",
    "                logo_with_features['features'] = features\n",
    "                analyzed_logos.append(logo_with_features)\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Add invalid features for failed analysis\n",
    "                logo_with_features = logo.copy()\n",
    "                logo_with_features['features'] = {'valid': False, 'error': str(e)}\n",
    "                analyzed_logos.append(logo_with_features)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Feature analysis completed: {successful_analysis}/{len(logos)} valid in {elapsed:.1f}s\")\n",
    "        \n",
    "        return analyzed_logos\n",
    "\n",
    "print(\"Fourier Logo Analyzer implemented with batch analysis support\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e225f800",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogoVisualizationPipeline:\n",
    "    \"\"\"Create visualizations for logo analysis results\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results_loaded = False\n",
    "        self.extraction_data = None\n",
    "        self.analyzed_logos = None\n",
    "        self.similar_pairs = None\n",
    "        self.clusters = None\n",
    "        \n",
    "    def load_results_from_memory(self, extraction_data, analyzed_logos, similar_pairs, clusters):\n",
    "        \"\"\"Load results from memory (for notebook use)\"\"\"\n",
    "        self.extraction_data = extraction_data\n",
    "        self.analyzed_logos = analyzed_logos\n",
    "        self.similar_pairs = similar_pairs\n",
    "        self.clusters = clusters\n",
    "        self.results_loaded = True\n",
    "        print(\"Results loaded into visualizer\")\n",
    "    \n",
    "    def create_extraction_performance_chart(self):\n",
    "        \"\"\"Create extraction performance visualization\"\"\"\n",
    "        if not self.results_loaded:\n",
    "            print(\" No results loaded\")\n",
    "            return\n",
    "            \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Extraction success breakdown\n",
    "        successful = len(self.extraction_data['successful_logos'])\n",
    "        total = len(self.extraction_data['websites'])\n",
    "        failed = total - successful\n",
    "        \n",
    "        # Create subplot layout\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('Logo Extraction Performance Analysis', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 1. Success Rate Pie Chart\n",
    "        labels = ['Successful', 'Failed']\n",
    "        sizes = [successful, failed]\n",
    "        colors = ['#2E8B57', '#DC143C']\n",
    "        \n",
    "        ax1.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "        ax1.set_title(f'Extraction Success Rate\\n({successful}/{total} websites)')\n",
    "        \n",
    "        # 2. Tier Usage (if available)\n",
    "        if hasattr(self.extraction_data['successful_logos'][0], 'tier_used'):\n",
    "            tier_counts = defaultdict(int)\n",
    "            for logo in self.extraction_data['successful_logos']:\n",
    "                tier = logo.get('tier_used', 'Unknown')\n",
    "                tier_counts[f\"Tier {tier}\"] += 1\n",
    "            \n",
    "            tiers = list(tier_counts.keys())\n",
    "            counts = list(tier_counts.values())\n",
    "            \n",
    "            ax2.bar(tiers, counts, color='#4682B4')\n",
    "            ax2.set_title('Success by API Tier')\n",
    "            ax2.set_ylabel('Number of Logos')\n",
    "            plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45)\n",
    "        \n",
    "        # 3. Logo File Sizes Distribution\n",
    "        sizes = []\n",
    "        for logo in self.extraction_data['successful_logos']:\n",
    "            if 'logo_data' in logo and logo['logo_data']:\n",
    "                sizes.append(len(logo['logo_data']))\n",
    "        \n",
    "        if sizes:\n",
    "            ax3.hist(sizes, bins=20, color='#FF6347', alpha=0.7)\n",
    "            ax3.set_title('Logo File Size Distribution')\n",
    "            ax3.set_xlabel('File Size (bytes)')\n",
    "            ax3.set_ylabel('Count')\n",
    "        \n",
    "        # 4. Feature Analysis Success\n",
    "        if self.analyzed_logos:\n",
    "            valid_features = sum(1 for logo in self.analyzed_logos if logo.get('features', {}).get('valid', False))\n",
    "            invalid_features = len(self.analyzed_logos) - valid_features\n",
    "            \n",
    "            ax4.bar(['Valid Features', 'Invalid Features'], [valid_features, invalid_features], \n",
    "                   color=['#32CD32', '#FF4500'])\n",
    "            ax4.set_title('Feature Extraction Success')\n",
    "            ax4.set_ylabel('Number of Logos')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('extraction_performance_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"Extraction performance chart created\")\n",
    "    \n",
    "    def create_similarity_analysis_chart(self):\n",
    "        \"\"\"Create similarity analysis visualization\"\"\"\n",
    "        if not self.results_loaded or not self.similar_pairs:\n",
    "            print(\"No similarity data available\")\n",
    "            return\n",
    "            \n",
    "        plt.figure(figsize=(14, 10))\n",
    "        \n",
    "        # Create subplot layout  \n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        fig.suptitle('Logo Similarity Analysis Dashboard', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Extract similarity scores\n",
    "        similarity_scores = [pair[2] for pair in self.similar_pairs if len(pair) >= 3]\n",
    "        \n",
    "        # 1. Similarity Score Distribution\n",
    "        if similarity_scores:\n",
    "            ax1.hist(similarity_scores, bins=20, color='#9370DB', alpha=0.7, edgecolor='black')\n",
    "            ax1.axvline(np.mean(similarity_scores), color='red', linestyle='--', \n",
    "                       label=f'Mean: {np.mean(similarity_scores):.3f}')\n",
    "            ax1.set_title('Similarity Score Distribution')\n",
    "            ax1.set_xlabel('Similarity Score')\n",
    "            ax1.set_ylabel('Number of Pairs')\n",
    "            ax1.legend()\n",
    "        \n",
    "        # 2. Cluster Size Distribution\n",
    "        if self.clusters:\n",
    "            cluster_sizes = [len(cluster) for cluster in self.clusters if len(cluster) > 1]\n",
    "            \n",
    "            if cluster_sizes:\n",
    "                ax2.hist(cluster_sizes, bins=max(10, len(set(cluster_sizes))), \n",
    "                        color='#20B2AA', alpha=0.7, edgecolor='black')\n",
    "                ax2.set_title('Brand Cluster Size Distribution')\n",
    "                ax2.set_xlabel('Cluster Size (websites)')\n",
    "                ax2.set_ylabel('Number of Clusters')\n",
    "            else:\n",
    "                ax2.text(0.5, 0.5, 'No multi-website clusters found', \n",
    "                        ha='center', va='center', transform=ax2.transAxes, fontsize=12)\n",
    "                ax2.set_title('Cluster Analysis')\n",
    "        \n",
    "        # 3. Top Similar Pairs\n",
    "        if similarity_scores:\n",
    "            top_pairs = sorted(self.similar_pairs, key=lambda x: x[2] if len(x) >= 3 else 0, reverse=True)[:10]\n",
    "            \n",
    "            pair_labels = []\n",
    "            scores = []\n",
    "            \n",
    "            for i, pair in enumerate(top_pairs):\n",
    "                website1 = pair[0].replace('https://', '').replace('http://', '').split('/')[0]\n",
    "                website2 = pair[1].replace('https://', '').replace('http://', '').split('/')[0] \n",
    "                \n",
    "                # Shorten domain names for display\n",
    "                domain1 = website1.split('.')[-2] if '.' in website1 else website1\n",
    "                domain2 = website2.split('.')[-2] if '.' in website2 else website2\n",
    "                \n",
    "                pair_labels.append(f\"{domain1}-{domain2}\")\n",
    "                scores.append(pair[2])\n",
    "            \n",
    "            if scores:\n",
    "                bars = ax3.barh(range(len(scores)), scores, color=plt.cm.viridis(np.linspace(0, 1, len(scores))))\n",
    "                ax3.set_yticks(range(len(scores)))\n",
    "                ax3.set_yticklabels(pair_labels)\n",
    "                ax3.set_title('Top 10 Most Similar Logo Pairs')\n",
    "                ax3.set_xlabel('Similarity Score')\n",
    "                \n",
    "                # Add value labels\n",
    "                for i, (bar, score) in enumerate(zip(bars, scores)):\n",
    "                    ax3.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,\n",
    "                            f'{score:.3f}', va='center', fontsize=9)\n",
    "        \n",
    "        # 4. Summary Statistics\n",
    "        stats_text = f\"\"\"Analysis Summary:\n",
    "        \n",
    " Total Websites: {len(self.extraction_data['websites'])}\n",
    " Successful Extractions: {len(self.extraction_data['successful_logos'])}\n",
    " Valid Features: {len(self.analyzed_logos) if self.analyzed_logos else 0}\n",
    " Similar Pairs Found: {len(self.similar_pairs)}\n",
    " Brand Clusters: {len([c for c in self.clusters if len(c) > 1]) if self.clusters else 0}\n",
    " Largest Cluster: {max(len(c) for c in self.clusters) if self.clusters else 0} logos\n",
    "\n",
    "Success Rates:\n",
    " Extraction: {len(self.extraction_data['successful_logos'])/len(self.extraction_data['websites'])*100:.1f}%\n",
    " Feature Analysis: {len(self.analyzed_logos)/len(self.extraction_data['successful_logos'])*100:.1f if self.analyzed_logos else 0}%\"\"\"\n",
    "        \n",
    "        ax4.text(0.05, 0.95, stats_text, transform=ax4.transAxes, fontsize=11,\n",
    "                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n",
    "        ax4.set_xlim(0, 1)\n",
    "        ax4.set_ylim(0, 1)\n",
    "        ax4.axis('off')\n",
    "        ax4.set_title('Pipeline Statistics')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('similarity_analysis_visualization.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"Similarity analysis chart created\")\n",
    "    \n",
    "    def create_cluster_dashboard(self):\n",
    "        \"\"\"Create comprehensive cluster analysis dashboard\"\"\"\n",
    "        if not self.results_loaded or not self.clusters:\n",
    "            print(\"No cluster data available\")\n",
    "            return\n",
    "            \n",
    "        # Filter multi-website clusters\n",
    "        multi_clusters = [cluster for cluster in self.clusters if len(cluster) > 1]\n",
    "        \n",
    "        if not multi_clusters:\n",
    "            print(\"No multi-website clusters found\")\n",
    "            return\n",
    "        \n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        fig.suptitle('Brand Cluster Analysis Dashboard', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        cluster_sizes = [len(cluster) for cluster in multi_clusters]\n",
    "        \n",
    "        # 1. Cluster Size Distribution\n",
    "        ax1.hist(cluster_sizes, bins=max(5, len(set(cluster_sizes))), \n",
    "                color='#FF6B6B', alpha=0.7, edgecolor='black')\n",
    "        ax1.set_title('Cluster Size Distribution')\n",
    "        ax1.set_xlabel('Cluster Size (websites)')\n",
    "        ax1.set_ylabel('Number of Clusters')\n",
    "        ax1.axvline(np.mean(cluster_sizes), color='blue', linestyle='--',\n",
    "                   label=f'Mean: {np.mean(cluster_sizes):.1f}')\n",
    "        ax1.legend()\n",
    "        \n",
    "        # 2. Top Clusters by Size\n",
    "        sorted_clusters = sorted(multi_clusters, key=len, reverse=True)[:10]\n",
    "        cluster_names = []\n",
    "        sizes = []\n",
    "        \n",
    "        for i, cluster in enumerate(sorted_clusters):\n",
    "            # Create a representative name from the first domain\n",
    "            sample_domain = cluster[0].replace('https://', '').replace('http://', '').split('/')[0]\n",
    "            brand_name = sample_domain.split('.')[0]\n",
    "            cluster_names.append(f\"Cluster {i+1}\\n({brand_name})\")\n",
    "            sizes.append(len(cluster))\n",
    "        \n",
    "        if sizes:\n",
    "            bars = ax2.bar(range(len(sizes)), sizes, color=plt.cm.Set3(np.linspace(0, 1, len(sizes))))\n",
    "            ax2.set_xticks(range(len(sizes)))\n",
    "            ax2.set_xticklabels(cluster_names, rotation=45, ha='right')\n",
    "            ax2.set_title('Top 10 Largest Brand Clusters')\n",
    "            ax2.set_ylabel('Number of Websites')\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar, size in zip(bars, sizes):\n",
    "                ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                        str(size), ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # 3. Clustering Efficiency Metrics\n",
    "        total_websites = len(self.extraction_data['websites'])\n",
    "        clustered_websites = sum(len(cluster) for cluster in multi_clusters)\n",
    "        single_clusters = len(self.clusters) - len(multi_clusters)\n",
    "        \n",
    "        metrics = {\n",
    "            'Multi-Brand Clusters': len(multi_clusters),\n",
    "            'Single Logo Clusters': single_clusters, \n",
    "            'Clustered Websites': clustered_websites,\n",
    "            'Unclustered Websites': total_websites - len(self.clusters)\n",
    "        }\n",
    "        \n",
    "        bars = ax3.bar(range(len(metrics)), list(metrics.values()),\n",
    "                      color=['#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7'])\n",
    "        ax3.set_xticks(range(len(metrics)))\n",
    "        ax3.set_xticklabels(list(metrics.keys()), rotation=45, ha='right')\n",
    "        ax3.set_title('Clustering Efficiency Metrics')\n",
    "        ax3.set_ylabel('Count')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, value in zip(bars, metrics.values()):\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(metrics.values())*0.01,\n",
    "                    str(value), ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # 4. Cluster Quality Summary\n",
    "        quality_text = f\"\"\"Cluster Quality Assessment:\n",
    "\n",
    " Total Brand Families: {len(multi_clusters)}\n",
    " Largest Brand Cluster: {max(cluster_sizes)} websites  \n",
    " Average Cluster Size: {np.mean(cluster_sizes):.1f} websites\n",
    " Smallest Brand Cluster: {min(cluster_sizes)} websites\n",
    "\n",
    " Coverage Metrics:\n",
    " Websites in Brand Clusters: {clustered_websites:,}\n",
    " Clustering Rate: {clustered_websites/total_websites*100:.1f}%\n",
    " Brand Discovery Rate: {len(multi_clusters)/total_websites*100:.2f}%\n",
    "\n",
    " Similarity Metrics:\n",
    " Similar Pairs Found: {len(self.similar_pairs)}\n",
    " Avg Pairs per Brand: {len(self.similar_pairs)/len(multi_clusters):.1f}\"\"\"\n",
    "\n",
    "        ax4.text(0.05, 0.95, quality_text, transform=ax4.transAxes, fontsize=10,\n",
    "                verticalalignment='top', \n",
    "                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "        ax4.set_xlim(0, 1) \n",
    "        ax4.set_ylim(0, 1)\n",
    "        ax4.axis('off')\n",
    "        ax4.set_title('Quality Assessment')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('cluster_analysis_dashboard.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"Cluster analysis dashboard created\")\n",
    "    \n",
    "    def create_all_visualizations(self):\n",
    "        \"\"\"Create all visualization charts\"\"\"\n",
    "        print(\"\\n CREATING COMPREHENSIVE VISUALIZATIONS\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        try:\n",
    "            self.create_extraction_performance_chart()\n",
    "            print()\n",
    "            self.create_similarity_analysis_chart()\n",
    "            print()\n",
    "            self.create_cluster_dashboard()\n",
    "            \n",
    "            print(f\"\\nAll visualizations completed!\")\n",
    "            print(\"Charts saved as PNG files in current directory\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Visualization error: {e}\")\n",
    "            print(\"Continuing without visualizations...\")\n",
    "\n",
    "print(\" LogoVisualizationPipeline class implemented!\")\n",
    "print(\"   - Extraction performance charts\") \n",
    "print(\"   - Similarity analysis dashboard\")\n",
    "print(\"   - Brand cluster analysis\")\n",
    "print(\"   - Memory-based loading for notebooks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e18b97e",
   "metadata": {},
   "source": [
    "# Enhanced Logo Similarity Analysis with SIFT/ORB\n",
    "\n",
    "## Multi-Method Approach for Maximum Accuracy\n",
    "\n",
    "This enhanced analysis combines **5 different similarity methods**:\n",
    "\n",
    "### **Fourier-Based Methods (Global Shape Analysis)**\n",
    "1. **pHash (DCT)** - Perceptual hashing for near-duplicate detection\n",
    "2. **FFT Features** - Low-frequency global shape signatures  \n",
    "3. **Fourier-Mellin** - Rotation and scale invariant matching\n",
    "\n",
    "### **Keypoint-Based Methods (Local Feature Analysis)**\n",
    "4. **SIFT Features** - Scale-Invariant Feature Transform for distinctive keypoints\n",
    "5. **ORB Features** - Oriented FAST and Rotated BRIEF for fast binary matching\n",
    "\n",
    "### **Why This Combination Works Better:**\n",
    "\n",
    "- **Fourier methods** excel at detecting logos with similar global shapes/patterns\n",
    "- **SIFT/ORB methods** excel at matching logos with distinctive keypoints (text, corners, unique shapes)\n",
    "- **Complementary strengths** cover different types of logo similarities:\n",
    "  - Similar color schemes  FFT\n",
    "  - Rotated/scaled versions  Fourier-Mellin + SIFT\n",
    "  - Different backgrounds  SIFT/ORB keypoints\n",
    "  - Partial logos  ORB features\n",
    "  \n",
    "### **Decision Logic:**\n",
    "**OR combination** - logos are similar if **ANY** method detects similarity, maximizing recall while maintaining precision through carefully tuned thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9423410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_complete_logo_analysis_pipeline(sample_size=None, max_tier=5, create_visuals=True):\n",
    "    \"\"\"\n",
    "    Complete end-to-end logo analysis pipeline with all enhancements\n",
    "    \n",
    "    Args:\n",
    "        sample_size: Number of websites to process (None for all in parquet)\n",
    "        max_tier: Maximum API tier to use (1-5, higher = more coverage but slower)\n",
    "        create_visuals: Whether to generate visualization charts\n",
    "    \n",
    "    Returns:\n",
    "        Complete analysis results with extraction, similarity, clustering, and visuals\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\" COMPLETE LOGO ANALYSIS PIPELINE WITH ALL ENHANCEMENTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # CHECK FOR EXISTING SAVED RESULTS\n",
    "    import os\n",
    "    if os.path.exists('logo_extraction_results.pkl'):\n",
    "        print(\"\\nFOUND EXISTING EXTRACTION RESULTS!\")\n",
    "        print(\"You can resume from saved data instead of re-extracting logos.\")\n",
    "        print(\"To resume: saved_data = load_saved_extraction_results()\")\n",
    "        print(\"          results = await resume_pipeline_from_extraction(saved_data)\")\n",
    "        print(\"Continuing with fresh extraction...\\n\")\n",
    "    \n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    # Step 1: Load Data\n",
    "    print(\"\\n DATA LOADING\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    df = LightningParquetProcessor.load_parquet_fast(\n",
    "        'logos.snappy.parquet', \n",
    "        sample_size=sample_size\n",
    "    )\n",
    "    \n",
    "    website_col = LightningParquetProcessor.get_website_column(df)\n",
    "    websites = df[website_col].dropna().tolist()\n",
    "    \n",
    "    print(f\" Processing {len(websites)} websites\")\n",
    "    \n",
    "    # Step 2: Enhanced Logo Extraction (targeting 97%+ success)\n",
    "    print(f\"\\n ENHANCED LOGO EXTRACTION (Max Tier: {max_tier})\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    async with EnhancedAPILogoExtractor() as extractor:\n",
    "        logo_results = await extractor.batch_extract_logos_enhanced(websites, max_tier=max_tier)\n",
    "    \n",
    "    successful_logos = [r for r in logo_results if r['logo_found']]\n",
    "    success_rate = len(successful_logos) / len(websites) * 100\n",
    "    \n",
    "    print(f\"Logo extraction: {len(successful_logos)}/{len(websites)} ({success_rate:.1f}% success)\")\n",
    "    \n",
    "    # SAVE EXTRACTION RESULTS IMMEDIATELY (prevent data loss)\n",
    "    print(f\"\\n SAVING EXTRACTION RESULTS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    import pickle\n",
    "    import json\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Save extraction results in multiple formats for safety\n",
    "    extraction_results = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'websites': websites,\n",
    "        'logo_results': logo_results,\n",
    "        'successful_logos': successful_logos,\n",
    "        'success_rate': success_rate,\n",
    "        'total_processed': len(websites),\n",
    "        'total_successful': len(successful_logos),\n",
    "        'max_tier_used': max_tier\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Save as pickle (preserves binary logo data)\n",
    "        with open('logo_extraction_results.pkl', 'wb') as f:\n",
    "            pickle.dump(extraction_results, f)\n",
    "        print(\" Saved extraction results as pickle file\")\n",
    "        \n",
    "        # Save metadata as JSON (human-readable backup)\n",
    "        json_safe_results = {\n",
    "            'timestamp': extraction_results['timestamp'],\n",
    "            'websites': extraction_results['websites'],\n",
    "            'success_rate': extraction_results['success_rate'],\n",
    "            'total_processed': extraction_results['total_processed'],\n",
    "            'total_successful': extraction_results['total_successful'],\n",
    "            'max_tier_used': extraction_results['max_tier_used'],\n",
    "            'successful_websites': [r['website'] for r in successful_logos],\n",
    "            'api_breakdown': {}\n",
    "        }\n",
    "        \n",
    "        # Add API service breakdown\n",
    "        api_counts = {}\n",
    "        for result in successful_logos:\n",
    "            service = result.get('api_service', 'Unknown')\n",
    "            api_counts[service] = api_counts.get(service, 0) + 1\n",
    "        json_safe_results['api_breakdown'] = api_counts\n",
    "        \n",
    "        with open('logo_extraction_metadata.json', 'w') as f:\n",
    "            json.dump(json_safe_results, f, indent=2)\n",
    "        print(\" Saved extraction metadata as JSON file\")\n",
    "        \n",
    "        print(f\" Extraction results safely stored in:\")\n",
    "        print(f\"   - logo_extraction_results.pkl (complete data)\")\n",
    "        print(f\"   - logo_extraction_metadata.json (summary)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Warning: Could not save extraction results: {e}\")\n",
    "        print(\"Continuing with pipeline...\")\n",
    "    \n",
    "    if len(successful_logos) < 2:\n",
    "        print(\"\\nNeed at least 2 logos for similarity analysis\")\n",
    "        print(\" Extraction results saved - you can resume later!\")\n",
    "        return extraction_results\n",
    "    \n",
    "    # Step 3: Fourier Feature Analysis\n",
    "    print(f\"\\n FOURIER FEATURE ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Use external classes if available, otherwise use notebook definitions\n",
    "    if USE_EXTERNAL_CLASSES:\n",
    "        print(\"Using FourierLogoAnalyzer from similarity_pipeline.py\")\n",
    "        analyzer = FourierLogoAnalyzer()\n",
    "    else:\n",
    "        print(\"Using notebook FourierLogoAnalyzer definition\")\n",
    "        analyzer = FourierLogoAnalyzer()\n",
    "    \n",
    "    analyzed_logos = analyzer.analyze_logo_batch(successful_logos)\n",
    "    valid_logos = [logo for logo in analyzed_logos if logo['features']['valid']]\n",
    "    \n",
    "    print(f\"Feature analysis: {len(valid_logos)}/{len(successful_logos)} logos with valid features\")\n",
    "    \n",
    "    if len(valid_logos) < 2:\n",
    "        print(\" Need at least 2 valid logos for similarity analysis\")\n",
    "        return None\n",
    "    \n",
    "    # Step 4: Similarity Analysis\n",
    "    print(f\"\\nSIMILARITY ANALYSIS\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    # Use existing FourierLogoAnalyzer directly (no wrapper needed!)\n",
    "    similar_pairs = analyzer.find_similar_pairs(analyzed_logos, threshold=0.7)\n",
    "    print(f\"Similarity analysis: {len(similar_pairs)} similar pairs found\")\n",
    "    \n",
    "    # Step 5: Union-Find Clustering\n",
    "    print(f\"\\nUNION-FIND CLUSTERING\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    if similar_pairs:\n",
    "        # Get all websites from valid logos\n",
    "        all_websites = [logo['website'] for logo in valid_logos]\n",
    "        \n",
    "        # Use existing UnionFind class directly \n",
    "        uf = UnionFind(all_websites)\n",
    "        \n",
    "        # Process similar pairs\n",
    "        for website1, website2, similarity in similar_pairs:\n",
    "            if website1 in all_websites and website2 in all_websites:\n",
    "                uf.union(website1, website2)\n",
    "        \n",
    "        clusters = uf.get_clusters()\n",
    "        union_trace = []  # Could add if needed\n",
    "        \n",
    "        # Filter multi-logo clusters\n",
    "        multi_clusters = [cluster for cluster in clusters if len(cluster) > 1]\n",
    "        print(f\" Clustering: {len(multi_clusters)} brand clusters discovered\")\n",
    "        \n",
    "        # Show largest clusters\n",
    "        if multi_clusters:\n",
    "            sorted_clusters = sorted(multi_clusters, key=len, reverse=True)[:5]\n",
    "            print(\" Top brand clusters:\")\n",
    "            for i, cluster in enumerate(sorted_clusters, 1):\n",
    "                sample_domain = cluster[0].replace('https://', '').replace('http://', '').split('/')[0]\n",
    "                brand_name = sample_domain.split('.')[0] if '.' in sample_domain else sample_domain\n",
    "                print(f\"   {i}. {brand_name}: {len(cluster)} similar logos\")\n",
    "    else:\n",
    "        clusters = [[logo['website']] for logo in valid_logos]  # Each logo in its own cluster\n",
    "        union_trace = []\n",
    "        print(\" No similar pairs found - each logo in separate cluster\")\n",
    "    \n",
    "    # Step 6: Create Visualizations\n",
    "    if create_visuals:\n",
    "        print(f\"\\nVISUALIZATION GENERATION\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        viz_pipeline = LogoVisualizationPipeline()\n",
    "        \n",
    "        # Prepare extraction results for visualization\n",
    "        extraction_data = {\n",
    "            'websites': websites,\n",
    "            'logo_results': logo_results,\n",
    "            'successful_logos': successful_logos\n",
    "        }\n",
    "        \n",
    "        # Load results into visualizer\n",
    "        viz_pipeline.load_results_from_memory(\n",
    "            extraction_data,\n",
    "            analyzed_logos, \n",
    "            similar_pairs,\n",
    "            clusters\n",
    "        )\n",
    "        \n",
    "        # Create all visualizations\n",
    "        viz_pipeline.create_all_visualizations()\n",
    "    \n",
    "    # Step 7: Summary Report\n",
    "    total_elapsed = time.time() - total_start_time\n",
    "    \n",
    "    print(f\"\\n PIPELINE COMPLETE!\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\" RESULTS SUMMARY:\")\n",
    "    print(f\"   - Websites processed: {len(websites)}\")\n",
    "    print(f\"   - Logos extracted: {len(successful_logos)} ({success_rate:.1f}% success)\")\n",
    "    print(f\"   - Valid features: {len(valid_logos)}\")\n",
    "    print(f\"   - Similar pairs: {len(similar_pairs)}\")\n",
    "    print(f\"   - Brand clusters: {len(clusters)}\")\n",
    "    print(f\"   - Processing time: {total_elapsed:.1f} seconds\")\n",
    "    print(f\"   - API tier used: 1-{max_tier}\")\n",
    "    \n",
    "    if success_rate >= 97:\n",
    "        print(f\" EXCELLENT! {success_rate:.1f}% success rate achieved!\")\n",
    "    elif success_rate >= 90:\n",
    "        print(f\" GREAT! {success_rate:.1f}% success rate\")\n",
    "    else:\n",
    "        print(f\" Consider increasing max_tier for better coverage\")\n",
    "    \n",
    "    # Return complete results\n",
    "    return {\n",
    "        'websites': websites,\n",
    "        'logo_results': logo_results,\n",
    "        'successful_logos': successful_logos,\n",
    "        'analyzed_logos': analyzed_logos,\n",
    "        'valid_logos': valid_logos,\n",
    "        'similar_pairs': similar_pairs,\n",
    "        'clusters': clusters,\n",
    "        'union_trace': union_trace if 'union_trace' in locals() else [],\n",
    "        'success_rate': success_rate,\n",
    "        'processing_time': total_elapsed,\n",
    "        'visualizations_created': create_visuals\n",
    "    }\n",
    "\n",
    "print(\" Complete integrated pipeline ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a848fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saved_extraction_results():\n",
    "    \"\"\"Load previously saved extraction results to resume pipeline\"\"\"\n",
    "    import pickle\n",
    "    import os\n",
    "    \n",
    "    try:\n",
    "        if os.path.exists('logo_extraction_results.pkl'):\n",
    "            with open('logo_extraction_results.pkl', 'rb') as f:\n",
    "                results = pickle.load(f)\n",
    "            \n",
    "            print(\"LOADED SAVED EXTRACTION RESULTS\")\n",
    "            print(\"=\" * 50)\n",
    "            print(f\"Timestamp: {results['timestamp']}\")\n",
    "            print(f\" Websites processed: {results['total_processed']}\")\n",
    "            print(f\"Successful extractions: {results['total_successful']}\")\n",
    "            print(f\"Success rate: {results['success_rate']:.1f}%\")\n",
    "            print(f\"Max tier used: {results['max_tier_used']}\")\n",
    "            \n",
    "            return results\n",
    "        else:\n",
    "            print(\"No saved extraction results found\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"rror loading saved results: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "async def resume_pipeline_from_extraction(saved_results, create_visuals=True):\n",
    "    \"\"\"Resume pipeline from saved extraction results\"\"\"\n",
    "    \n",
    "    print(\"\\nRESUMING PIPELINE FROM SAVED EXTRACTION RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Extract data from saved results\n",
    "    websites = saved_results['websites']\n",
    "    logo_results = saved_results['logo_results']\n",
    "    successful_logos = saved_results['successful_logos']\n",
    "    success_rate = saved_results['success_rate']\n",
    "    \n",
    "    print(f\"Resuming with {len(successful_logos)} successfully extracted logos...\")\n",
    "    \n",
    "    if len(successful_logos) < 2:\n",
    "        print(\" Need at least 2 logos for similarity analysis\")\n",
    "        return saved_results\n",
    "    \n",
    "    # Continue with Step 3: Fourier Feature Analysis\n",
    "    print(f\"\\n FOURIER FEATURE ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Use external classes if available, otherwise use notebook definitions\n",
    "    if USE_EXTERNAL_CLASSES:\n",
    "        print(\"Using FourierLogoAnalyzer from similarity_pipeline.py\")\n",
    "        analyzer = FourierLogoAnalyzer()\n",
    "    else:\n",
    "        print(\"Using notebook FourierLogoAnalyzer definition\")\n",
    "        analyzer = FourierLogoAnalyzer()\n",
    "    \n",
    "    analyzed_logos = analyzer.analyze_logo_batch(successful_logos)\n",
    "    valid_logos = [logo for logo in analyzed_logos if logo['features']['valid']]\n",
    "    \n",
    "    print(f\"Feature analysis: {len(valid_logos)}/{len(successful_logos)} logos with valid features\")\n",
    "    \n",
    "    if len(valid_logos) < 2:\n",
    "        print(\"Need at least 2 valid logos for similarity analysis\")\n",
    "        return {**saved_results, 'analyzed_logos': analyzed_logos, 'valid_logos': valid_logos}\n",
    "    \n",
    "    # Step 4: Similarity Analysis\n",
    "    print(f\"\\n SIMILARITY ANALYSIS\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    similar_pairs = analyzer.find_similar_pairs(analyzed_logos, threshold=0.7)\n",
    "    print(f\"Similarity analysis: {len(similar_pairs)} similar pairs found\")\n",
    "    \n",
    "    # Step 5: Union-Find Clustering\n",
    "    print(f\"\\n UNION-FIND CLUSTERING\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    if similar_pairs:\n",
    "        all_websites = [logo['website'] for logo in valid_logos]\n",
    "        uf = UnionFind(all_websites)\n",
    "        \n",
    "        for website1, website2, similarity in similar_pairs:\n",
    "            if website1 in all_websites and website2 in all_websites:\n",
    "                uf.union(website1, website2)\n",
    "        \n",
    "        clusters = uf.get_clusters()\n",
    "        multi_clusters = [cluster for cluster in clusters if len(cluster) > 1]\n",
    "        print(f\"Clustering: {len(multi_clusters)} brand clusters discovered\")\n",
    "        \n",
    "        if multi_clusters:\n",
    "            sorted_clusters = sorted(multi_clusters, key=len, reverse=True)[:5]\n",
    "            print(\"Top brand clusters:\")\n",
    "            for i, cluster in enumerate(sorted_clusters, 1):\n",
    "                sample_domain = cluster[0].replace('https://', '').replace('http://', '').split('/')[0]\n",
    "                brand_name = sample_domain.split('.')[0] if '.' in sample_domain else sample_domain\n",
    "                print(f\"   {i}. {brand_name}: {len(cluster)} similar logos\")\n",
    "    else:\n",
    "        clusters = [[logo['website']] for logo in valid_logos]\n",
    "        print(\"No similar pairs found - each logo in separate cluster\")\n",
    "    \n",
    "    # Step 6: Create Visualizations\n",
    "    if create_visuals:\n",
    "        print(f\"\\n VISUALIZATION GENERATION\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        viz_pipeline = LogoVisualizationPipeline()\n",
    "        \n",
    "        extraction_data = {\n",
    "            'websites': websites,\n",
    "            'logo_results': logo_results,\n",
    "            'successful_logos': successful_logos\n",
    "        }\n",
    "        \n",
    "        viz_pipeline.load_results_from_memory(\n",
    "            extraction_data,\n",
    "            analyzed_logos, \n",
    "            similar_pairs,\n",
    "            clusters\n",
    "        )\n",
    "        \n",
    "        viz_pipeline.create_all_visualizations()\n",
    "    \n",
    "    # Complete results\n",
    "    complete_results = {\n",
    "        **saved_results,  # Include original extraction results\n",
    "        'analyzed_logos': analyzed_logos,\n",
    "        'valid_logos': valid_logos,\n",
    "        'similar_pairs': similar_pairs,\n",
    "        'clusters': clusters,\n",
    "        'union_trace': [],\n",
    "        'visualizations_created': create_visuals,\n",
    "        'pipeline_completed': True\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n PIPELINE RESUMED AND COMPLETED!\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    return complete_results\n",
    "\n",
    "\n",
    "print(\"Data persistence utilities ready!\")\n",
    "print(\"   - Auto-save after logo extraction\")\n",
    "print(\"   - Resume pipeline from saved results\")\n",
    "print(\"   - Multiple backup formats (pickle + JSON)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d815d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  EXAMPLE: How to use the recovery features\n",
    "\n",
    "print(\" DATA RECOVERY & RESUME EXAMPLES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n1 RESUME FROM SAVED EXTRACTION:\")\n",
    "print(\"# If your pipeline failed after logo extraction, you can resume:\")\n",
    "print(\"saved_data = load_saved_extraction_results()\")\n",
    "print(\"if saved_data:\")\n",
    "print(\"    results = await resume_pipeline_from_extraction(saved_data)\")\n",
    "\n",
    "print(\"\\n2 CHECK FOR EXISTING DATA:\")\n",
    "print(\"# Before running full pipeline, check if you have saved data:\")\n",
    "print(\"if os.path.exists('logo_extraction_results.pkl'):\")\n",
    "print(\"    print('Found saved extraction results!')\")\n",
    "print(\"    # Option to load and continue vs restart\")\n",
    "\n",
    "print(\"\\n3 SELECTIVE PROCESSING:\")\n",
    "print(\"# You can also run just extraction with different settings:\")\n",
    "print(\"# results = await run_complete_logo_analysis_pipeline(\")\n",
    "print(\"#     sample_size=100,    # Start small\")\n",
    "print(\"#     max_tier=3,         # Use fewer tiers for speed\")\n",
    "print(\"#     create_visuals=False # Skip visuals for now\")\n",
    "print(\"# )\")\n",
    "\n",
    "print(\"\\n DATA SAFETY FEATURES:\")\n",
    "print(\" Auto-save after logo extraction\")\n",
    "print(\" Multiple backup formats (pickle + JSON)\")\n",
    "print(\" Resume capability from any saved point\")\n",
    "print(\" Timestamped results\")\n",
    "print(\" Detailed metadata logging\")\n",
    "\n",
    "print(\"\\n FILES CREATED:\")\n",
    "print(\"- logo_extraction_results.pkl (complete data with binary logos)\")\n",
    "print(\"- logo_extraction_metadata.json (human-readable summary)\")\n",
    "\n",
    "print(\"\\nReady to use! Run your pipeline with confidence! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8b51ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 3: Full Production Pipeline (ALL websites from parquet)\n",
    "print(\"\\nOPTION 3: Full Production Pipeline - Process ALL websites in parquet file\")\n",
    "print(\"This will process all websites in the parquet file (may take several minutes)\")\n",
    "print(\"Uncomment the code below when ready for full production run:\")\n",
    "\n",
    "# Uncomment for full production run:\n",
    "full_results = await run_complete_logo_analysis_pipeline(\n",
    "    sample_size=None,     # Process ALL websites in parquet\n",
    "    max_tier=5,           # Use all API tiers for maximum success rate\n",
    "    create_visuals=True   # Generate comprehensive visualizations\n",
    ")\n",
    "\n",
    "print(\"\\nPipeline configurations ready!\")\n",
    "print(\"Choose the option that fits your needs and run the cell\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
